---
title: 'Quick Start Guide'
description: 'Get started with OpenOps in minutes'
icon: 'rocket'
---

import AutoInstall from '/snippets/auto-install.mdx'
import OpenOpsCloudLimitations from '/snippets/openops-cloud-limitations.mdx'

## Deploy OpenOps on your local machine  

1. Make sure you have a [supported operating system](/getting-started/system-requirements#operating-systems) and a [compatible Docker](/getting-started/system-requirements#docker) client.
2. Run the following command in your terminal to install OpenOps:   
    `curl -fsS https://openops.sh/install | sh `  

    You can use the same command also to update, configure and run OpenOps.  
 <AutoInstall/>

3. Once the installation is done, it provides you with the following:
    1. URL to your OpenOps instance (typically `http://localhost/`),
    2. admin username and,
    3. autogenerated password.  

    To easily access OpenOps later, you might want to bookmark the URL in your browser.

## Run OpenOps from your browser   

1. Open your browser and then OpenOps.  
![First start of OpenOps](/images/qsg-first-start.png)  
2. Use the admin credentials you got after installation to log in. 

## Get familiar with OpenOps  

After logging in, **Overview** opens up:
![Initial screen](/images/qsg-initial-screen.png)

In the top-left corner, click the **Open sidebar** icon to expand the main OpenOps navigation menu. In a fresh installation, the following views contain little content, but knowing they exist can be helpful:
* **Workflows** keep track of the workflows you create.
* **Runs** keep a history of workflow executions, indicating start time and status.
* **Connections** list all permissions you provide to OpenOps for various cloud providers and services, as well as let you create new ones.
* **Tables** store data collected by workflows, as well as any information you enter or import to use in your workflows.
* **Analytics** let you visualize data from OpenOps tables and external data sources, such as CSV files or Google Sheets.

## Customize and test a sample workflow

**Overview** suggests three sample beginner-level workflows to help you get a feel of the product. Let's use one of them to collect all available AWS Compute Optimizer recommendations for a variety of AWS resource types, and save the recommendations to an [OpenOps table](/reporting-analytics/tables/).  

1. Click **AWS sample workflow**.   
![AWS sample workflow](/images/qsg-aws-sample-workflow.png)  

2. On the workflow preview page, click **Use template**.    
![AWS sample workflow - use template](/images/qsg-aws-workflow-preview.png)  

3. On the workflow summary page, create a connection to AWS by clicking **+ Add**.  
![AWS sample workflow summary](/images/qsg-aws-workflow-connections.png)  

4. Configure the AWS connection by entering three required parameters: **Access Key ID**, **Secret Access Key** and **Default Region**.
![Create AWS connection](/images/qsg-aws-create-connection.png)   

    To get an access key ID and a secret access key, [create an IAM user](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) in the AWS Management Console or use one of your existing IAM users. Make sure your IAM user has the following permissions to run this workflow:

    ```text
    ec2:Describe*
    sts:GetCallerIdentity
    organizations:Describe*
    organizations:List*
    compute-optimizer:Get*
    compute-optimizer:Describe*
    lambda:List*
    autoscaling:Describe*
    ```

    If your AWS account doesn't already have a default region, you can [set it in the AWS Management Console](https://docs.aws.amazon.com/
    awsconsolehelpdocs/latest/gsg/change-default-region.html), and then enter it in the connection form.
    When you're done, save your entries.   

5. On the workflow summary page, now that the connection is defined, click **Create workflow**.  

    The workflow editor displays the steps of the workflow as a block diagram.  
    ![Workflow editor](/images/qsg-aws-workflow-editor.png)  

    The first step is a trigger: a schedule or event that initiates the workflow execution. In this workflow, the trigger schedules
    workflow to run every working day at 7AM UTC. All other steps are actions, and each action represents some kind of automation that the workflow performs.  

    Click **Tree view** and **Notes** in the top menu bar to get a compact view of the workflow and a detailed description of what it does.  
    ![Tree view and notes](/images/qgs-aws-tree-view-and-notes.png)  

    Explore the workflow by clicking on blocks that represent its steps. This shows properties of each step in the right pane of the workflow editor.  
    ![Step properties in the workflow editor](/images/qsg-aws-step-properties.png)  

6. Test the workflow before it goes live.  
    
    Testing the workflow runs it in full, so you can treat it as an on-demand run in addition to its scheduled runs.

    At the top of the workflow editor, there's the **Test workflow** button. However, at first it's grayed out since you have to test your trigger first.  
    ![Test the trigger first](/images/qgs-aws-test-the-trigger-first.png)

    1. Click the trigger block in the workflow graph. 
    2. In the trigger's properties pane, click the **Test** tab, then click **Load Data** in the **Step output** section.
        Test results are displayed.  
        ![Trigger test success](/images/qsg-aws-trigger-tested.png)  
    3. Now that the **Test workflow** button is active, go ahead and click it.  
        OpenOps will take a few seconds to run the workflow. **Run Details** are displayed in the left pane of the workflow editor.  
        ![Run Details pane](/images/qsg-aws-run-details.png)  

        This pane displays the status of each workflow step. If you click a step, you can see its inputs and outputs as JSON in the bottom of the pane. As long as the status of each step is green, your workflow is ready to go and you can publish it.

7. Publish the workflow to start running the workflow on schedule.  

    When testing, the workflow editor switches to read-only mode. To get back to editing mode, click **Edit** on the top right.
    Once back in the editing mode, the top-right button is now called **Publish**. Once you click it, your workflow is live. It will now run periodically on the schedule defined by its trigger.  

8. See the results of your test run.   

    When you tested the workflow, it was a full on-demand run. Some of the workflow steps fetched AWS Compute Optimizer recommendations for various AWS resource types, and other steps recorded these recommendations to an OpenOps table. Let's see what's been recorded to the table.

    1. Click the book icon on the top left to show the sidebar.  
    2. In the sidebar, click **Tables** to see a list of all predefined tables under **OpenOps dataset**.  
    ![Tables in the sidebar](/images/qsg-sidebar-tables.png)  
    3. Click **Opportunities** to see a table of the saving opportunities that the workflow run revealed, including estimates in USD.  
    ![Opportunities](/images/qsg-opportunities.png)


## Get templates for your future workflows

The sample AWS workflow that you've just tested and published provides just a quick glance at what OpenOps can do.

In real FinOps scenarios, you would want your workflows to do more, for example:  
   - find owners of unused resources,   
   - request decision on whether to delete or keep resources,   
   - if required to delete, execute IaC automations and create GitHub pull requests.   

Fortunately, OpenOps provides dozens of real-world FinOps templates that can serve as a base for your workflows. Just click **Overview** and  then **Explore templates** in the top-right menu bar to see the **template catalog**.  
![Template catalog](/images/template-catalog.png)

By default, the catalog shows six templates. To view more, click **Explore more** and sign up for a free OpenOps Cloud account.  

<OpenOpsCloudLimitations/>

As a result, the **template catalog** in your OpenOps installation will be extended to show all available templates.  
![OpenOps template catalog after signing up with OpenOps Cloud](/images/template-catalog-all-templates.png)  

Click any template in the catalog to see its full description, a preview diagram visualizing the workflow steps defined in the template, and the integrations that the template uses.  
![CloudWatch log retention enforcement template](/images/template-log-retention.png)  

If you find a template useful, click **Use template** to create a workflow based on it, just like you did earlier with the sample AWS workflow.

## Connect OpenOps to your AI provider

Building steps in OpenOps workflows is usually intuitive. Still, when you need to enter a cloud-specific command, write an SQL query, or add a custom script, you might need assistance.  

OpenOps lets you connect your preferred LLM provider, such as OpenAI or Anthropic, so you can generate queries, scripts, and CLI commands directly within the workflow editor.  

    1. At the bottom of the sidebar, click **Settings**, then **AI**.   
    2. Choose your LLM provider and select a model.  
    3. Enter your API key, and click **Save**.   

Once connected, any time you edit a step that involves a command, query, or custom code, you can click **Generate with AI** to prompt your LLM and generate it for you.  
![AI chat window](/images/access-llm-chat.png)

## Join the OpenOps community

This was just a glimpse of what OpenOps can do for you to help you automate your FinOps practices.  

As you continue to build new workflows, you might consider joining the [OpenOps community Slack](https://slack.openops.com/) to get direct support from the OpenOps team, collaborate with other FinOps professionals, and receive help in building your workflows. You're also welcome to share your feedback to help shape the future of OpenOps.
